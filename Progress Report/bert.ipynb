{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"00Fkv8gvR8Mi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666854166100,"user_tz":-480,"elapsed":4172,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"f97111b7-0089-4a9e-833a-aba97ce8205a"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install transformers\n","!pip install shap"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"poL0nMjkSodH","executionInfo":{"status":"ok","timestamp":1666861261985,"user_tz":-480,"elapsed":12147,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"60d30dcc-7086-47f6-b312-472045d9a936"},"execution_count":94,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting shap\n","  Downloading shap-0.41.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (569 kB)\n","\u001b[K     |████████████████████████████████| 569 kB 14.3 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from shap) (1.0.2)\n","Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.7/dist-packages (from shap) (21.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from shap) (1.21.6)\n","Collecting slicer==0.0.7\n","  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from shap) (1.3.5)\n","Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap) (0.56.3)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from shap) (1.7.3)\n","Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap) (4.64.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>20.9->shap) (3.0.9)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->shap) (57.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba->shap) (4.13.0)\n","Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap) (0.39.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->shap) (3.9.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba->shap) (4.1.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->shap) (2022.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->shap) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->shap) (3.1.0)\n","Installing collected packages: slicer, shap\n","Successfully installed shap-0.41.0 slicer-0.0.7\n"]}]},{"cell_type":"code","execution_count":95,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":95}],"source":["# Python libraries\n","# Data Science modules\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","plt.style.use('ggplot')\n","from collections import defaultdict\n","from textwrap import wrap\n","import shap\n","\n","# Import nltk modules and download dataset\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.util import ngrams\n","from nltk.tokenize import word_tokenize\n","\n","# Import Scikit-learn moduels\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.metrics import accuracy_score, f1_score, plot_confusion_matrix\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn import model_selection\n","from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate, StratifiedKFold, learning_curve, RandomizedSearchCV\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import confusion_matrix, classification_report\n","# import scikit-plot as skplt\n","\n","\n","# huggingface and torch\n","from torch import nn, optim\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","stop = set(stopwords.words('english'))\n","\n","RANDOM_SEED = 42\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"K5Ph7aqVRrI_","outputId":"4167a99a-243d-49ee-b474-830e14938178","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666861680980,"user_tz":-480,"elapsed":2240,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}}},{"cell_type":"code","source":["colab_path= \"/content/drive/Othercomputers/My MacBook Pro/data-science/Georgia Tech/MGT6203- Data Analytics Business/MGT6203 Project/\"\n","finphrase_dir = \"data/FinancialPhraseBank/\"\n","filename = 'Sentences_50Agree.txt'\n"],"metadata":{"id":"KCGDdsV4RuQH","executionInfo":{"status":"ok","timestamp":1666854170215,"user_tz":-480,"elapsed":40,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","execution_count":36,"metadata":{"collapsed":true,"pycharm":{"name":"#%%\n"},"id":"BL47S2wTRrJD","executionInfo":{"status":"ok","timestamp":1666854170215,"user_tz":-480,"elapsed":39,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}},"outputs":[],"source":["\n","def load_finphrase(filename):\n","    ''' Clean FinancialPhrasebank data\n","        Input:\n","            - filename\n","        Output:\n","            - a dataframe for the loaded financial phase bank data\n","    '''\n","    df = pd.read_csv(colab_path+ finphrase_dir + filename,\n","                     sep='\\@',\n","                     engine='python',\n","                     header=None,\n","                     names=['sentence','label'],\n","                     encoding='latin-1')\n","    print('Total number of record in the file: ', df.shape[0])\n","    df.drop_duplicates(inplace=True)\n","    print('Total number of record after dropping duplicates: ', df.shape[0])\n","    print('Missing label: ', df['label'].isnull().sum())\n","    df.reset_index(inplace=True, drop=True)\n","    # df = pd.get_dummies(df, columns=['label'])\n","    return df"]},{"cell_type":"code","execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Total number of record in the file:  4846\n","Total number of record after dropping duplicates:  4840\n","Missing label:  0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n","  after removing the cwd from sys.path.\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                                                                                                                                                                                                                                                                      sentence  \\\n","3200  The company serves customers in various industries , including process and resources , industrial machinery , architecture , building , construction , electrical , transportation , electronics , chemical , petrochemical , energy , and information technology , as well as catering and households .   \n","2527  Officials did not disclose the contract value .                                                                                                                                                                                                                                                            \n","4101  The extracted filtrates are very high in clarity while the dried filter cakes meet required transport moisture limits (TMLs)for their ore grades .                                                                                                                                                         \n","1926  The tool is a patent pending design that allows consumers to lay out their entire project on a removable plate using multiple clear stamps of any kind .                                                                                                                                                   \n","1536  In Finland , the corresponding service is Alma Media 's Etuovi.com , Finland 's most popular and best known nationwide online service for home and property sales .                                                                                                                                        \n","1330  Sanoma also has an Executive Committee , in accordance with the Company 's Articles of Association , that prepares proposals for matters to be decided or noted by the Board of Directors .                                                                                                                \n","2373  Finnish property investment company Citycon plans to issue directed subordinated convertible bonds to institutional investors .                                                                                                                                                                            \n","4251  The gross area of the Innova 2 project will be about 10,000 sq m ( 107,600 sq ft ) .                                                                                                                                                                                                                       \n","\n","        label  \n","3200  neutral  \n","2527  neutral  \n","4101  neutral  \n","1926  neutral  \n","1536  neutral  \n","1330  neutral  \n","2373  neutral  \n","4251  neutral  "],"text/html":["\n","  <div id=\"df-c762b3cf-719f-4aca-ad5d-e4e05d2866f1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>3200</th>\n","      <td>The company serves customers in various industries , including process and resources , industrial machinery , architecture , building , construction , electrical , transportation , electronics , chemical , petrochemical , energy , and information technology , as well as catering and households .</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>2527</th>\n","      <td>Officials did not disclose the contract value .</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>4101</th>\n","      <td>The extracted filtrates are very high in clarity while the dried filter cakes meet required transport moisture limits (TMLs)for their ore grades .</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1926</th>\n","      <td>The tool is a patent pending design that allows consumers to lay out their entire project on a removable plate using multiple clear stamps of any kind .</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1536</th>\n","      <td>In Finland , the corresponding service is Alma Media 's Etuovi.com , Finland 's most popular and best known nationwide online service for home and property sales .</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1330</th>\n","      <td>Sanoma also has an Executive Committee , in accordance with the Company 's Articles of Association , that prepares proposals for matters to be decided or noted by the Board of Directors .</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>2373</th>\n","      <td>Finnish property investment company Citycon plans to issue directed subordinated convertible bonds to institutional investors .</td>\n","      <td>neutral</td>\n","    </tr>\n","    <tr>\n","      <th>4251</th>\n","      <td>The gross area of the Innova 2 project will be about 10,000 sq m ( 107,600 sq ft ) .</td>\n","      <td>neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c762b3cf-719f-4aca-ad5d-e4e05d2866f1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c762b3cf-719f-4aca-ad5d-e4e05d2866f1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c762b3cf-719f-4aca-ad5d-e4e05d2866f1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":37}],"source":["df = load_finphrase(filename)\n","\n","# Samples\n","pd.set_option('display.max_colwidth', -1)\n","df.sample(n=8, random_state=42)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"MZg0GgJTRrJE","outputId":"f2e79bee-dd6c-4ea6-f1fe-d5d9d0b5c0d2","colab":{"base_uri":"https://localhost:8080/","height":540},"executionInfo":{"status":"ok","timestamp":1666854170215,"user_tz":-480,"elapsed":38,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}}},{"cell_type":"code","execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["['negative', 'neutral', 'positive']\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                                                                                                                                                                                               sentence  \\\n","0  According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .                                                                                                        \n","1  Technopolis plans to develop in stages an area of no less than 100,000 square meters in order to host companies working in computer technologies and telecommunications , the statement said .                                         \n","2  The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility ; contrary to earlier layoffs the company contracted the ranks of its office workers , the daily Postimees reported .   \n","3  With the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profitability .                         \n","4  According to the company 's updated strategy for the years 2009-2012 , Basware targets a long-term net sales growth in the range of 20 % -40 % with an operating profit margin of 10 % -20 % of net sales .                            \n","\n","   label  \n","0  1      \n","1  1      \n","2  0      \n","3  2      \n","4  2      "],"text/html":["\n","  <div id=\"df-de57ee92-bcd6-4c36-8b8d-edef314c06e8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Technopolis plans to develop in stages an area of no less than 100,000 square meters in order to host companies working in computer technologies and telecommunications , the statement said .</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The international electronic industry company Elcoteq has laid off tens of employees from its Tallinn facility ; contrary to earlier layoffs the company contracted the ranks of its office workers , the daily Postimees reported .</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>With the new production plant the company would increase its capacity to meet the expected increase in demand and would improve the use of raw materials and therefore increase the production profitability .</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>According to the company 's updated strategy for the years 2009-2012 , Basware targets a long-term net sales growth in the range of 20 % -40 % with an operating profit margin of 10 % -20 % of net sales .</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-de57ee92-bcd6-4c36-8b8d-edef314c06e8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-de57ee92-bcd6-4c36-8b8d-edef314c06e8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-de57ee92-bcd6-4c36-8b8d-edef314c06e8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":38}],"source":["class_names = ['negative', 'neutral', 'positive']\n","\n","# Encode the label\n","le = LabelEncoder()\n","le.fit(df['label'])\n","print(list(le.classes_))\n","df['label'] = le.transform(df['label'])\n","# list(le.inverse_transform(df['label']))\n","df.head()"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"qC2GLT1zRrJE","outputId":"435ae3b3-2f0c-465f-da63-df28693ab9d8","colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"status":"ok","timestamp":1666854170216,"user_tz":-480,"elapsed":29,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}}},{"cell_type":"code","source":["\n","ax = sns.countplot(df.label)\n","plt.xlabel('review sentiment')\n","ax.set_xticklabels(class_names);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":344},"id":"iVHHgWABk4-_","executionInfo":{"status":"ok","timestamp":1666859657690,"user_tz":-480,"elapsed":434,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"cc832650-220f-436c-c3e9-c8b768ad9f51"},"execution_count":85,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n","  FutureWarning\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":[" Sentence: When was I last outside? I am stuck at home for 2 weeks.\n","   Tokens: ['When', 'was', 'I', 'last', 'outside', '?', 'I', 'am', 'stuck', 'at', 'home', 'for', '2', 'weeks', '.']\n","Token IDs: [1332, 1108, 146, 1314, 1796, 136, 146, 1821, 5342, 1120, 1313, 1111, 123, 2277, 119]\n"]}],"source":["# Data Preprocessing\n","\n","PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n","tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)\n","sample_txt = 'When was I last outside? I am stuck at home for 2 weeks.'\n","tokens = tokenizer.tokenize(sample_txt)\n","token_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","print(f' Sentence: {sample_txt}')\n","print(f'   Tokens: {tokens}')\n","print(f'Token IDs: {token_ids}')"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"WSSiRyKFRrJF","outputId":"6c441fc2-73ba-49bc-821c-778cb6ec4529","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666854170216,"user_tz":-480,"elapsed":26,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}}},{"cell_type":"code","execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['input_ids', 'attention_mask'])"]},"metadata":{},"execution_count":40}],"source":["encoding = tokenizer.encode_plus(\n","  sample_txt,\n","  truncation=True,\n","  max_length=32,\n","  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n","  return_token_type_ids=False,\n","  padding=True,\n","  return_attention_mask=True,\n","  return_tensors='pt',  # Return PyTorch tensors\n",")\n","\n","encoding.keys()"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"ooBEaOpaRrJF","outputId":"c7f4a318-ce4a-462a-e441-55db21d0a176","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666854170216,"user_tz":-480,"elapsed":24,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}}},{"cell_type":"code","execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["17\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([ 101, 1332, 1108,  146, 1314, 1796,  136,  146, 1821, 5342, 1120, 1313,\n","        1111,  123, 2277,  119,  102])"]},"metadata":{},"execution_count":41}],"source":["print(len(encoding['input_ids'][0]))\n","encoding['input_ids'][0]"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"IjqQqk_VRrJG","outputId":"30fda323-1bbc-49f9-c7ab-a6789681877e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666854170217,"user_tz":-480,"elapsed":23,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}}},{"cell_type":"code","execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["17\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"]},"metadata":{},"execution_count":42}],"source":["print(len(encoding['attention_mask'][0]))\n","encoding['attention_mask']"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"yQI4RxO2RrJG","outputId":"25b1a602-3aac-44b1-e86d-32fb108d5a43","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666854170217,"user_tz":-480,"elapsed":22,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}}},{"cell_type":"code","execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]',\n"," 'When',\n"," 'was',\n"," 'I',\n"," 'last',\n"," 'outside',\n"," '?',\n"," 'I',\n"," 'am',\n"," 'stuck',\n"," 'at',\n"," 'home',\n"," 'for',\n"," '2',\n"," 'weeks',\n"," '.',\n"," '[SEP]']"]},"metadata":{},"execution_count":43}],"source":["tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"IKV-Yk0ARrJH","outputId":"7096978b-05e9-41e0-a3ab-cb00f318bc4d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666854170217,"user_tz":-480,"elapsed":21,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}}},{"cell_type":"code","execution_count":92,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n","  warnings.warn(msg, FutureWarning)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["# Choosing Sequence Length\n","\n","token_lens = []\n","\n","for txt in df['sentence']:\n","  tokens = tokenizer.encode(txt, truncation=True, max_length=512)\n","  token_lens.append(len(tokens))\n","\n","sns.distplot(token_lens)\n","plt.xlim([0, 256])\n","plt.xlabel('Token count')\n","\n","MAX_LEN = 160\n","\n","# mostly below 90 token count"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"dZvCuvoDRrJH","colab":{"base_uri":"https://localhost:8080/","height":341},"executionInfo":{"status":"ok","timestamp":1666860829948,"user_tz":-480,"elapsed":5573,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"b1f0b077-d104-482b-c323-80634691168e"}},{"cell_type":"code","execution_count":45,"outputs":[],"source":["class GPReviewDataset(Dataset):\n","\n","  def __init__(self, reviews, targets, tokenizer, max_len):\n","    self.reviews = reviews\n","    self.targets = targets\n","    self.tokenizer = tokenizer\n","    self.max_len = max_len\n","\n","  def __len__(self):\n","    return len(self.reviews)\n","\n","  def __getitem__(self, item):\n","    review = str(self.reviews[item])\n","    target = self.targets[item]\n","\n","    encoding = self.tokenizer.encode_plus(\n","      review,\n","      add_special_tokens=True,\n","      max_length=self.max_len,\n","      return_token_type_ids=False,\n","      pad_to_max_length=True,\n","      return_attention_mask=True,\n","      return_tensors='pt',\n","    )\n","\n","    return {\n","      'review_text': review,\n","      'input_ids': encoding['input_ids'].flatten(),\n","      'attention_mask': encoding['attention_mask'].flatten(),\n","      'targets': torch.tensor(target, dtype=torch.long)\n","    }\n","\n","\n","def create_data_loader(df, tokenizer, max_len, batch_size):\n","  ds = GPReviewDataset(\n","    reviews=df['sentence'].to_numpy(),\n","    targets=df['label'].to_numpy(),\n","    tokenizer=tokenizer,\n","    max_len=max_len\n","  )\n","\n","  return DataLoader(\n","    ds,\n","    batch_size=batch_size,\n","    num_workers=4\n","  )"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"yhMVpBDNRrJH","executionInfo":{"status":"ok","timestamp":1666854170217,"user_tz":-480,"elapsed":19,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}}},{"cell_type":"code","execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((4356, 2), (242, 2), (242, 2))"]},"metadata":{},"execution_count":46}],"source":["df_train, df_test = train_test_split(df, test_size=0.1, random_state=RANDOM_SEED)\n","df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)\n","df_train.shape, df_val.shape, df_test.shape"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"FRRsJq9vRrJI","outputId":"2305365d-5de2-4611-89a3-ccabd505c313","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666854170217,"user_tz":-480,"elapsed":19,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}}},{"cell_type":"code","execution_count":47,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}],"source":["BATCH_SIZE = 16\n","\n","train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n","val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n","test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"mefR6j3rRrJI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666854170218,"user_tz":-480,"elapsed":19,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"c616d97e-6be5-4977-e5d9-ea0a4e309070"}},{"cell_type":"code","execution_count":48,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"]},"metadata":{},"execution_count":48}],"source":["data = next(iter(train_data_loader))\n","data.keys()"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"VZgHBvG9RrJI","outputId":"6d9268f0-4f5e-489f-e53e-40c612dd7e55","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666854170784,"user_tz":-480,"elapsed":584,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}}},{"cell_type":"code","execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 160])\n","torch.Size([16, 160])\n","torch.Size([16])\n"]}],"source":["print(data['input_ids'].shape)\n","print(data['attention_mask'].shape)\n","print(data['targets'].shape)"],"metadata":{"pycharm":{"name":"#%%\n"},"id":"DMy2rTj-RrJJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666854170784,"user_tz":-480,"elapsed":4,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"ddf1da62-d4c2-4dc7-e330-9758e25429e7"}},{"cell_type":"markdown","source":["### model"],"metadata":{"id":"PiMLYDhQRrJJ"}},{"cell_type":"code","source":["bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjJOiiPPTe6A","executionInfo":{"status":"ok","timestamp":1666854172920,"user_tz":-480,"elapsed":2138,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"87b503eb-30bd-400b-ba17-0d1266c26113"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["last_hidden_state, pooled_output = bert_model(\n","  input_ids=encoding['input_ids'], \n","  attention_mask=encoding['attention_mask']\n",")\n","\n","last_hidden_state.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yj_k11UaTJLW","executionInfo":{"status":"ok","timestamp":1666854172920,"user_tz":-480,"elapsed":7,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"d1ee7425-1f9c-4f9a-ed0a-5dfd6bf93aa1"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 17, 768])"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["bert_model.config.hidden_size"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jnVg4kWQTP8s","executionInfo":{"status":"ok","timestamp":1666854172920,"user_tz":-480,"elapsed":5,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"c88520d0-c20f-4125-8fe0-f9a88aa1e0dc"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["768"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["pooled_output.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SpKSBeVVTQ9w","executionInfo":{"status":"ok","timestamp":1666854172920,"user_tz":-480,"elapsed":4,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"0855ec65-d643-4257-fe74-b3ab9d0bb8d9"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 768])"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["class SentimentClassifier(nn.Module):\n","\n","  def __init__(self, n_classes):\n","    super(SentimentClassifier, self).__init__()\n","    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, return_dict=False)\n","    self.drop = nn.Dropout(p=0.3)\n","    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n","  \n","  def forward(self, input_ids, attention_mask):\n","    _, pooled_output = self.bert(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask\n","    )\n","    output = self.drop(pooled_output)\n","    return self.out(output)"],"metadata":{"id":"k2N9Bbx2TSY5","executionInfo":{"status":"ok","timestamp":1666854172921,"user_tz":-480,"elapsed":4,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["model = SentimentClassifier(len(class_names))\n","model = model.to(device)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o7fKi9B6TTmj","executionInfo":{"status":"ok","timestamp":1666854174952,"user_tz":-480,"elapsed":2035,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"9a9318b5-a5ce-4c39-a973-270f280d46df"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["input_ids = data['input_ids'].to(device)\n","attention_mask = data['attention_mask'].to(device)\n","\n","print(input_ids.shape) # batch size x seq length\n","print(attention_mask.shape) # batch size x seq length"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4doL6mJ9TUmM","executionInfo":{"status":"ok","timestamp":1666854174952,"user_tz":-480,"elapsed":6,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"3f668423-1ec7-4b70-d31e-cac3ef0d056c"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([16, 160])\n","torch.Size([16, 160])\n"]}]},{"cell_type":"code","source":["F.softmax(model(input_ids, attention_mask), dim=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-__AJSs6TVj3","executionInfo":{"status":"ok","timestamp":1666854174953,"user_tz":-480,"elapsed":6,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"31dc1e77-cb35-443b-ae37-c853a3e1f187"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.3694, 0.1928, 0.4378],\n","        [0.3158, 0.2236, 0.4606],\n","        [0.4833, 0.1350, 0.3817],\n","        [0.4017, 0.2151, 0.3832],\n","        [0.4075, 0.1212, 0.4713],\n","        [0.3580, 0.2145, 0.4275],\n","        [0.3972, 0.1943, 0.4085],\n","        [0.4759, 0.3168, 0.2073],\n","        [0.4424, 0.1476, 0.4100],\n","        [0.2099, 0.2414, 0.5487],\n","        [0.3772, 0.2248, 0.3980],\n","        [0.4185, 0.1666, 0.4149],\n","        [0.6171, 0.1397, 0.2433],\n","        [0.3630, 0.2165, 0.4205],\n","        [0.3495, 0.2215, 0.4291],\n","        [0.4170, 0.2182, 0.3648]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"]},"metadata":{},"execution_count":57}]},{"cell_type":"markdown","source":["### train"],"metadata":{"id":"E53P-_T1UgMo"}},{"cell_type":"code","source":["#%%\n","EPOCHS = 10\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","total_steps = len(train_data_loader) * EPOCHS\n","\n","scheduler = get_linear_schedule_with_warmup(\n","  optimizer,\n","  num_warmup_steps=0,\n","  num_training_steps=total_steps\n",")\n","\n","loss_fn = nn.CrossEntropyLoss().to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X2gAKBAxXDlu","executionInfo":{"status":"ok","timestamp":1666854174953,"user_tz":-480,"elapsed":4,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"30fbbe89-34d6-43d5-fcbc-9efad1b3e1a8"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n","  model = model.train()\n","\n","  losses = []\n","  correct_predictions = 0\n","  \n","  for d in data_loader:\n","    input_ids = d[\"input_ids\"].to(device)\n","    attention_mask = d[\"attention_mask\"].to(device)\n","    targets = d[\"targets\"].to(device)\n","\n","    outputs = model(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask\n","    )\n","\n","    _, preds = torch.max(outputs, dim=1)\n","    loss = loss_fn(outputs, targets)\n","\n","    correct_predictions += torch.sum(preds == targets)\n","    losses.append(loss.item())\n","\n","    loss.backward()\n","    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()\n","\n","  return correct_predictions.double() / n_examples, np.mean(losses)\n","\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","  model = model.eval()\n","\n","  losses = []\n","  correct_predictions = 0\n","\n","  with torch.no_grad():\n","    for d in data_loader:\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      targets = d[\"targets\"].to(device)\n","\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask\n","      )\n","      _, preds = torch.max(outputs, dim=1)\n","\n","      loss = loss_fn(outputs, targets)\n","\n","      correct_predictions += torch.sum(preds == targets)\n","      losses.append(loss.item())\n","\n","  return correct_predictions.double() / n_examples, np.mean(losses)"],"metadata":{"id":"2pdyJMEcUfZu","executionInfo":{"status":"ok","timestamp":1666854174953,"user_tz":-480,"elapsed":3,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","history = defaultdict(list)\n","best_accuracy = 0\n","\n","for epoch in range(EPOCHS):\n","\n","  print(f'Epoch {epoch + 1}/{EPOCHS}')\n","  print('-' * 10)\n","\n","  train_acc, train_loss = train_epoch(\n","    model,\n","    train_data_loader,    \n","    loss_fn, \n","    optimizer, \n","    device, \n","    scheduler, \n","    len(df_train)\n","  )\n","\n","  print(f'Train loss {train_loss} accuracy {train_acc}')\n","\n","  val_acc, val_loss = eval_model(\n","    model,\n","    val_data_loader,\n","    loss_fn, \n","    device, \n","    len(df_val)\n","  )\n","\n","  print(f'Val   loss {val_loss} accuracy {val_acc}')\n","  print()\n","\n","  history['train_acc'].append(train_acc)\n","  history['train_loss'].append(train_loss)\n","  history['val_acc'].append(val_acc)\n","  history['val_loss'].append(val_loss)\n","\n","  if val_acc > best_accuracy:\n","    torch.save(model.state_dict(), 'best_model_state.bin')\n","    best_accuracy = val_acc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ed7xaTafXLPq","executionInfo":{"status":"ok","timestamp":1666855325609,"user_tz":-480,"elapsed":1150659,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"00783c0e-c7bd-4e91-c44a-650e82e58f37"},"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.5454164519906044 accuracy 0.7786960514233241\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 0.4562944518402219 accuracy 0.8305785123966942\n","\n","Epoch 2/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.26506644378214966 accuracy 0.9118457300275482\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 0.6259139338508248 accuracy 0.8140495867768596\n","\n","Epoch 3/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.14777423137365653 accuracy 0.9584481175390266\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 0.9132615169510245 accuracy 0.7975206611570248\n","\n","Epoch 4/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.08684847576180371 accuracy 0.9777318640955005\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 0.8950383928386145 accuracy 0.8181818181818182\n","\n","Epoch 5/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.042119800862911644 accuracy 0.988751147842057\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 1.0364240678027272 accuracy 0.8140495867768596\n","\n","Epoch 6/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.025692115165229252 accuracy 0.9928833792470156\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 1.226493107670649 accuracy 0.8057851239669421\n","\n","Epoch 7/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.015231296566522667 accuracy 0.9960973370064279\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 1.2069100919688935 accuracy 0.8099173553719008\n","\n","Epoch 8/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.011237431155044787 accuracy 0.9972451790633609\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 1.2764992861139035 accuracy 0.8099173553719008\n","\n","Epoch 9/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.008019870136128466 accuracy 0.9979338842975206\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 1.2563294021906586 accuracy 0.8181818181818182\n","\n","Epoch 10/10\n","----------\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Train loss 0.005691779117001222 accuracy 0.9979338842975206\n"]},{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"stream","name":"stdout","text":["Val   loss 1.2521077346609673 accuracy 0.8099173553719008\n","\n","CPU times: user 12min 46s, sys: 6min 9s, total: 18min 55s\n","Wall time: 19min 10s\n"]}]},{"cell_type":"code","source":["plt.plot(history['train_acc'], label='train accuracy')\n","plt.plot(history['val_acc'], label='validation accuracy')\n","\n","plt.title('Training history')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend()\n","plt.ylim([0, 1]);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":751},"id":"61bU-B6aXUEE","executionInfo":{"status":"error","timestamp":1666861022325,"user_tz":-480,"elapsed":1090,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"1ae016ba-a167-4616-b5ae-5fea490886e7"},"execution_count":93,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1626\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1627\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1628\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'values'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-93-0d8ecf63c1ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training history'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2761\u001b[0m     return gca().plot(\n\u001b[1;32m   2762\u001b[0m         *args, scalex=scalex, scaley=scaley, **({\"data\": data} if data\n\u001b[0;32m-> 2763\u001b[0;31m         is not None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1645\u001b[0m         \"\"\"\n\u001b[1;32m   1646\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1648\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36mindex_of\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/cbook/__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     '''\n\u001b[1;32m   1325\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["## evaluation"],"metadata":{"id":"mH2zciPgXYmb"}},{"cell_type":"code","source":["test_acc, _ = eval_model(\n","  model,\n","  test_data_loader,\n","  loss_fn,\n","  device,\n","  len(df_test)\n",")\n","\n","test_acc.item()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lsG2p-ZQXaDC","executionInfo":{"status":"ok","timestamp":1666855912321,"user_tz":-480,"elapsed":2834,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"50f59d2a-dfbb-446d-df1f-39c2bb0b8913"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]},{"output_type":"execute_result","data":{"text/plain":["0.8842975206611571"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","source":["def get_predictions(model, data_loader):\n","  model = model.eval()\n","  \n","  review_texts = []\n","  predictions = []\n","  prediction_probs = []\n","  real_values = []\n","\n","  with torch.no_grad():\n","    for d in data_loader:\n","\n","      texts = d[\"review_text\"]\n","      input_ids = d[\"input_ids\"].to(device)\n","      attention_mask = d[\"attention_mask\"].to(device)\n","      targets = d[\"targets\"].to(device)\n","\n","      outputs = model(\n","        input_ids=input_ids,\n","        attention_mask=attention_mask\n","      )\n","      _, preds = torch.max(outputs, dim=1)\n","\n","      probs = F.softmax(outputs, dim=1)\n","\n","      review_texts.extend(texts)\n","      predictions.extend(preds)\n","      prediction_probs.extend(probs)\n","      real_values.extend(targets)\n","\n","  predictions = torch.stack(predictions).cpu()\n","  prediction_probs = torch.stack(prediction_probs).cpu()\n","  real_values = torch.stack(real_values).cpu()\n","  return review_texts, predictions, prediction_probs, real_values"],"metadata":{"id":"I1zKYudAWtXb","executionInfo":{"status":"ok","timestamp":1666855921869,"user_tz":-480,"elapsed":706,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n","  model,\n","  test_data_loader\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cBpGPvIgWvEq","executionInfo":{"status":"ok","timestamp":1666855937641,"user_tz":-480,"elapsed":2973,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"e4b08d04-18ed-497d-df51-95011bf3e317"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2308: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["print(classification_report(y_test, y_pred, target_names=class_names))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IRNTJwkMWxUR","executionInfo":{"status":"ok","timestamp":1666855977906,"user_tz":-480,"elapsed":2,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"c99cbd6c-b6bc-4c81-df6f-fe4d22ebcae2"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","    negative       0.94      0.91      0.92        33\n","     neutral       0.88      0.92      0.90       134\n","    positive       0.86      0.81      0.84        75\n","\n","    accuracy                           0.88       242\n","   macro avg       0.89      0.88      0.89       242\n","weighted avg       0.88      0.88      0.88       242\n","\n"]}]},{"cell_type":"code","source":["def show_confusion_matrix(confusion_matrix):\n","  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n","  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n","  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n","  plt.ylabel('True sentiment')\n","  plt.xlabel('Predicted sentiment');\n","\n","cm = confusion_matrix(y_test, y_pred)\n","df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n","show_confusion_matrix(df_cm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":308},"id":"RAf-HsoigeTF","executionInfo":{"status":"ok","timestamp":1666858479288,"user_tz":-480,"elapsed":939,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"d44bcbab-6a35-473c-c8e6-19a683179e88"},"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["idx = 2\n","\n","review_text = y_review_texts[idx]\n","true_sentiment = y_test[idx]\n","pred_df = pd.DataFrame({\n","  'class_names': class_names,\n","  'values': y_pred_probs[idx]\n","})"],"metadata":{"id":"kRUBefzdghQW","executionInfo":{"status":"ok","timestamp":1666858490743,"user_tz":-480,"elapsed":401,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["print(\"\\n\".join(wrap(review_text)))\n","print()\n","print(f'True sentiment: {class_names[true_sentiment]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09AX9Jnwgi_o","executionInfo":{"status":"ok","timestamp":1666858525753,"user_tz":-480,"elapsed":391,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"1a54d99f-9c7a-454c-b7e2-52d8f41534df"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["( ADP News ) - Nov 5 , 2008 - Finnish electronic measurement products\n","and solutions maker Vaisala Oyj ( OMX : VAIAS ) said today that its\n","net profit rose to EUR 18 million ( USD 23.1 m ) for the first nine\n","months of 2008 from EUR 1\n","\n","True sentiment: positive\n"]}]},{"cell_type":"code","source":["sns.barplot(x='values', y='class_names', data=pred_df, orient='h')\n","plt.ylabel('sentiment')\n","plt.xlabel('probability')\n","plt.xlim([0, 1]);"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":285},"id":"iaXql169g0ws","executionInfo":{"status":"ok","timestamp":1666858570570,"user_tz":-480,"elapsed":645,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"4b456b34-ee0d-4034-e6dc-f9700da2f0dd"},"execution_count":74,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["review_text = \"FB EPS dropped by 5% \""],"metadata":{"id":"WLDoIlgUg3aY","executionInfo":{"status":"ok","timestamp":1666858616570,"user_tz":-480,"elapsed":391,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}}},"execution_count":78,"outputs":[]},{"cell_type":"code","source":["encoded_review = tokenizer.encode_plus(\n","  review_text,\n","  max_length=MAX_LEN,\n","  add_special_tokens=True,\n","  return_token_type_ids=False,\n","  pad_to_max_length=True,\n","  return_attention_mask=True,\n","  return_tensors='pt',\n",")\n","\n","input_ids = encoded_review['input_ids'].to(device)\n","attention_mask = encoded_review['attention_mask'].to(device)\n","\n","output = model(input_ids, attention_mask)\n","_, prediction = torch.max(output, dim=1)\n","\n","print(f'Review text: {review_text}')\n","print(f'Sentiment  : {class_names[prediction]}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rA0VcYJ9g4dB","executionInfo":{"status":"ok","timestamp":1666858617848,"user_tz":-480,"elapsed":5,"user":{"displayName":"Galen Hew","userId":"14593423718388120675"}},"outputId":"fe03fbb8-9b16-4530-ba76-03ba9ab389d7"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stdout","text":["Review text: FB EPS dropped by 5% \n","Sentiment  : negative\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}