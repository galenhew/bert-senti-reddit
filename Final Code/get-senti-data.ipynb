{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNMm09TeoK8I4Oy8JVN/NDf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AR_Vmm-gUMyZ"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install praw\n","!pip install ffn\n","!pip install --upgrade pandas-datareader"],"metadata":{"id":"RB8t1n1_URng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# libraries\n","import numpy as np\n","import pandas as pd\n","pd.set_option('display.max_rows', 300)\n","import praw #reddit data api\n","from praw.models import MoreComments # module to get replies to comments\n","import ffn #for loading financial data\n","import matplotlib as mpl\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sn\n","import requests\n","import json\n","import csv\n","import time\n","import datetime\n","import warnings \n","warnings.filterwarnings(\"ignore\")\n","from tqdm import tqdm\n","import pickle"],"metadata":{"id":"BbqCn853US3M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# enter path\n","path = \"\""],"metadata":{"id":"JeK2ahJUUXVR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sub='wallstreetbets' #Subreddit to scrape\n","#start and end date of scraping\n","before = \"2021-12-31\"\n","after = \"2021-01-01\" # starting to scrape in august 2018 when the post first appeared\n","# define the thread we want to scrape every day\n","query = \"Daily Discussion Thread\"\n","subStats = []\n","subCount = 0\n","\n","\n","#function to get reddit post titles and urls with timestamp from pusshift api\n","def getPushshiftData(query, after, before, sub):\n","    url = ('https://api.pushshift.io/reddit/search/submission/?title='\n","           +str(query)+'&size=10000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)) # get Pusshift url\n","    r = requests.get(url) # get request\n","    data = json.loads(r.text) # load data into a json file\n","    return data['data'] # return part of the json file\n","\n","\n","#get needed data from data scraped above\n","def collectPosts(post):\n","    subData = ([post['id'], post['title'], post['url'], \n","                datetime.datetime.fromtimestamp(post['created_utc']).date()]) #create list to hold data about posts\n","    try:\n","        flair = post['link_flair_text'] # try to get flair of the post\n","    except KeyError:\n","        flair = \"NaN\" # if there is no flai return NaN\n","    subData.append(flair) # append flair\n","    subStats.append(subData) # append data \n","\n","    \n","data = getPushshiftData(query, after, before, sub) # get data\n","\n","\n","# loop will run until all posts have been gathered \n","# from the 'after' date up until before date\n","while len(data) > 0:\n","    for submission in data:\n","        collectPosts(submission)\n","        subCount+=1\n","    # Calls getPushshiftData() with the created date of the last submission\n","    after = data[-1]['created_utc']\n","    data = getPushshiftData(query, after, before, sub)\n","    \n","\n","#organize data into dataframe\n","# create variables\n","data={} # dictionary in preperation for dataframe\n","# lists that will contain column values\n","ids=[]\n","titles=[]\n","urls=[]\n","dates=[]\n","flairs=[]\n","\n","# get data into variables\n","for stat in subStats:\n","    ids.append(stat[0])\n","    titles.append(stat[1])\n","    urls.append(stat[2])\n","    dates.append(stat[3])\n","    flairs.append(stat[4])\n","\n","# append dictionary    \n","data['id']=ids\n","data['title']=titles\n","data['url']=urls\n","data['date']=dates\n","data['flair']=flairs\n","\n","# create dataframe from dictionary\n","\n","posts=pd.DataFrame(data)\n","posts=posts[posts['flair']=='Daily Discussion']\n","posts"],"metadata":{"id":"_L1EOINPUYd2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# setting up redit client to scrape comments of the posts with PRAW\n","reddit = praw.Reddit(\n","  client_id = \"\",\n","  client_secret = \"\",\n","  user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36\"\n",")"],"metadata":{"id":"R-HS4u8HUaOQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# long run time take note\n","\n","daily_comments=[] # list that will hold lists of daily comments\n","for url in tqdm((posts['url'].tolist())): # iterate over urls\n","    try:\n","        comments = []\n","        submission = reddit.submission(url=url) # get comments\n","        submission.comments.replace_more(limit=0) # this PRAW function allows to access comments and replies \n","        for comment in submission.comments: #\n","            comments.append(comment.body) # append comments and replies to list\n","        daily_comments.append(comments)\n","    except:\n","        comment=None\n","        comments.append(comment)\n","        "],"metadata":{"id":"NtgaF7c7Uc9I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check\n","comments[0]\n","daily_comments[1]"],"metadata":{"id":"Y3atzWPUUkFc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# store\n","posts.to_pickle(path + 'data/posts.pickle')\n","\n","with open(path + 'data/comments.pickle', 'wb') as fp:\n","  pickle.dump(comments, fp)\n","\n","with open(path + 'data/daily_comments.pickle', 'wb') as fp:\n","  pickle.dump(daily_comments, fp)\n","\n","with open (path + 'data/comments.pickle', 'rb') as fp:\n","  abc = pickle.load(fp)\n","\n","with open (path + 'data/daily_comments.pickle', 'rb') as fp:\n","  cde = pickle.load(fp)"],"metadata":{"id":"ctowjLchUm2Y"},"execution_count":null,"outputs":[]}]}